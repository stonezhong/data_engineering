<p>
    I am collecting best practices for Data Lake design and implementation.
</p>

Here is a list of high level goals you may consider when building a data lake:

<ul>
    <li><a href="#data_security">Data Security</a></li>
    <li><a href="#processing_data_at_scale">Processing Data at Scale</a></li>
    <li><a href="#streaming_and_or_batching_process">Streaming and/or Batching Process</a></li>
    <li><a href="#data_visualization">Data Visualization</a></li>
    <li><a href="#ETL_Pipeline_Orchestration">ETL Pipeline Orchestration</a></li>
    <li><a href="#ETL_Pipeline_Orchestration">ETL Pipeline Orchestration</a></li>
</ul>


<h2 id="data_security">Data Security</h2>
<p>
You want the data only be accessed by the authorized people, you also want keep an audit log, so you
know which asset being access, by whom and when.
</p>

<h2 id="processing_data_at_scale">Processing Data at Scale</h2>
<p>
As business grows, your data may grow as well. Most likely, you will need a platform which allows you to process data at scale,
for example, scanning a table of 1 billion rows of transactions and calculate the revenue generated by all the merchants.

Most likely, you will need some sort of big data based processing engine, for example, Apache Spark or Snowflake. Traditional Analytical Database
won't be able to fulfill this task eventually.

<ul>
    related technologies:
    <li>Apache Spark</li>
    <li>Various Cloud based Spark as service, e.g,: databricks, AWS EMR, Google Dataproc, Azure HD Insight, etc</li>
    <li>Snowflake</li>
</ul>

</p>


<h2 id="streaming_and_or_batching_process">Streaming and/or Batching Process</h2>
<p>
Based on the nature of your business, you may need to process some of your data in batch more while some other data processing
need to be in real time (or close to real time).

For example, a stock broker send trading summary to everyone at the end of day, it is a typical batching process.
However, bank's ATM system might monitor all the cred card transaction and may suspend potential fraudulent transaction
immediately as it discovers, the sooner you processed the data and discover the fraudulent transaction, the more loss you can prevent.

<ul>
    related technologies:
    <li>Apache Spark</li>
    <li>Apache Spark - streaming</li>
    <li>Apache Flink</li>
    <li>Apache Strom</li>
</ul>

</p>

<h2 id="data_visualization">Data Visualization</h2>
<p>
Visualized data is easier to understand, and get vision from it. For example, looking at sotck chart is better
than looking at 5-minute trading record of a stock, with the chart, you can see the trend better than looking at the table itself.

People sometimes also call it BI, or Business Intelligence

<ul>
    related technologies:
    <li>Apache Superset</li>
    <li>Microsoft Power BI</li>
    <li>Tableau</li>
</ul>
</p>


<h2 id="ETL_Pipeline_Orchestration">ETL Pipeline Orchestration</h2>
<p>

<ul>
    You need a system to orchestrate your ETL jobs, it means:
    <li>
        ETL jobs has dependency, you are responsible to define the order between ETL jobs, and system follows the order you defined to execute ETL jobs.
    </li>
    <li>
        It provide a way so you can recover from a failed ETL job.
    </li>
</ul>

<ul>
    related technologies:
    <li>Apache Airflow</li>
</ul>

</p>
